{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from math import e, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data,test_size = .25):\n",
    "    np.random.shuffle(data)\n",
    "    test = data[int(round((len(data)*test_size))):]\n",
    "    train = data[:(int(round(len(data)*test_size)))]\n",
    "    test_X = test[:,:-1]\n",
    "    test_y = test[:,-1:]\n",
    "    train_X = train[:,:-1]\n",
    "    train_y =  train[:,-1:]\n",
    "    return train_X,train_y,test_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    computes the sigmoid of z.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.e**(-z))\n",
    "\n",
    "def loss(X,y,theta):\n",
    "    \"\"\"\n",
    "    Loss function for logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    z = np.dot(X,theta)\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    return (1/m)*sum(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "\n",
    "def compute_cost(X, y, theta, lam = 0):\n",
    "    m = len(y)\n",
    "    \n",
    "    J = (1/m) * sum( (-y) * np.log( sigmoid(X.dot(theta)) ) - (1-y) * np.log( 1-sigmoid(X.dot(theta)) ) ) + ( (lam/(2*m)) * sum(theta[1:len(theta)]**2)) \n",
    "    \n",
    "    return J\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, epochs, lam = 0):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros([epochs,1])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        theta = theta*(1-(alpha*(lam/m))) - alpha * (1/m) * np.dot(X.T,(sigmoid(X.dot(theta)) - y))\n",
    "        \n",
    "        \n",
    "        cost_history[i] = compute_cost(X, y, theta, lam)\n",
    "\n",
    "    return theta, cost_history\n",
    "    \n",
    "    \n",
    "def logistic_regression(X,y,theta = None,alpha = .01,epochs=500,lam=0):\n",
    "\n",
    "    m = len(y) # number of samples\n",
    "\n",
    "    # declare intercept \n",
    "    X = np.append(np.ones([m,1]),X,axis=1)\n",
    "\n",
    "    # initialize theta parameters if not given\n",
    "    if theta == None:\n",
    "        theta = np.zeros([X.shape[1],1])\n",
    "    \n",
    "    print(f\"Running Gradient Descent with {epochs} iterations and a learning rate of {alpha}...\")\n",
    "\n",
    "    theta, cost_history = gradient_descent(X, y, theta, alpha, epochs, lam)\n",
    "\n",
    "    print(f\"Computed theta parameters are: \\n{theta}\")\n",
    "    \n",
    "    return theta,cost_history\n",
    "\n",
    "def predict(inputs,theta):\n",
    "    # adds the intercept value to the data the user is predicting if it is not there\n",
    "    if inputs.shape[1] != theta.shape[0]:\n",
    "        inputs = np.append(np.ones([len(inputs),1]),inputs,axis=1)\n",
    "    return [round(sigmoid(d)[0]) for d in inputs.dot(theta)]\n",
    "\n",
    "def accuracy(preds,y):\n",
    "    return 1-(sum(1 for i, j in zip(y, preds) if i != j)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('../data/binary_class_bank_note.csv',delimiter=',',skip_header=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y,test_X,test_y = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradient Descent with 500 iterations and a learning rate of 0.01...\n",
      "Computed theta parameters are: \n",
      "[[ 0.30623893]\n",
      " [-0.94238736]\n",
      " [-0.46461729]\n",
      " [-0.44283557]\n",
      " [-0.13670968]]\n"
     ]
    }
   ],
   "source": [
    "theta,cost_history = logistic_regression(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9640427599611273"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(test_X,theta)\n",
    "accuracy(preds,test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
